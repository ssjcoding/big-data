package ml.classify.DecisionTree;

/**
 * 概念定义：
 *
 * 		熵：度量随机变量的不确定性（纯度），熵越大，样本的不确定性就越大。
 *
 * 		信息增益（ID3算法）：
 * 			定义：以某特征划分数据集前后的熵的差值，使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。
 *				划分前样本集合D的熵是一定的，entroy(前)，
 *				使用某个特征A划分数据集D，计算划分后的数据子集的熵entroy(后)
 *					信息增益 = entroy(前) - entroy(后)	[ g(D,A) = H(D) - H(D/A) ]
 *			缺点：信息增益偏向取值越多的特征
 *			原因：当特征的取值较多时，根据次特征划分更容易得到纯度更高的子集，因此划分之后熵更低，由于划分前的熵是一定的，因此信息增益更大，因此信息增益比较偏向取值较多的特征。
 *
 *		信息增益比（C4.5算法）：
 *			公式：信息增益比 = 惩罚参数 * 信息增益	[ gR(D,A) = g(D,A)/HA(D) ]
 *				其中的HA(D)，对于样本集合D，将当前特征A作为随机变量（取值是特征A的各个特征值），求得的经验熵。
 *			信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。
 *		 	惩罚参数：数据集D以特征A作为随机变量的熵的倒数，即：将特征A取值相同的样本划分到同一个子集中（之前所说数据集的熵是依据类别进行划分的）
 *		 	缺点：信息增益比偏向取值较小的特征
 *		 	原因：当特征取值较少时HA(D)的值较小，因此其倒数较大，因为信息增益比较大。因而偏向取值较少的特征。
 *		 	使用信息增益比：基于以上缺点，并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。
 *
 *		基尼指数（CART算法 --- 分类与回归树）
 *			定义：基尼指数（基尼不纯度）：标识在样本集合中一个随机选中的样本被分错的概率。
 *				注意：Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。
 *			公式：基尼指数（基尼不纯度） = 样本被选中的概率 * 样本被分错的概率
 *			说明：CART是个二叉树，也就是当使用某个特征划分样本集合只有两个集合：1. 等于给定的特征值 的样本集合D1 ， 2 不等于给定的特征值 的样本集合D2
 *			所有的可能划分的Gini(D,Ai)中找出Gini指数最小的划分，这个划分的划分点，便是使用特征A对样本集合D进行划分的最佳划分点。
 *
 *		ID3、C4.5生成算法：
 *			输入：训练数据集D，特征集A，阈值ε；
 *			输出：决策树T
 *			（1）如果D中所有实例属于同一类Ck,则置T为单节点数，并将Ck作为该节点的类，返回T；
 *			（2）如果A=∅，则置T为单节点数，并将D中实例数最大的类Ck作为该节点的类，返回T；
 *			（3）否则，计算A中个特征对D的信息增益（ID3）或者信息增益比（C4.5），选择信息增益最大或者信息增益比最大的特征Ag；
 *			（4）如果Ag的信息增益比小于阈值	ε，则置T为单节点树，并将D中实例数最大的类Ck作为该节点的类，返回T；
 *			（5）否则，对Ag的每一个可能值ai，依Ag=ai将D分割为子集若干非空Di，将Di中实例数最大的类作为标记，构建子节点，由节点及其子结点构成树T，返回T；
 *			（6）对结点i，以Di为训练集，以A-{Ag}为特征集，递归地调用步（1）~步（5），得到子树Tj，返回Tj。
 *
 *		ID3和C4.5用于分类，CART可用于分类与回归
 *
 * @author tonysu,
 * @version 1.0v.
 * @Create 2018/7/24 下午5:44,
 */
public class DecisionTree{
}
